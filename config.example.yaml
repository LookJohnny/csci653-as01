# DarkHorse Configuration File
# Copy this to config.yaml and customize for your environment

# ============================================================================
# Data Paths
# ============================================================================
data:
  # Working directory for staging downloaded data
  work_dir: "/scratch/${USER}/amazon_stage"

  # Output directory for processed data
  out_dir: "/scratch/${USER}/amazon_out"

  # Raw data source (if using local copy)
  raw_data_dir: "/data/amazon2023"

# ============================================================================
# Data Processing
# ============================================================================
processing:
  # Number of rows per chunk for streaming
  rows_per_chunk: 1500000

  # Number of threads for DuckDB
  threads: 32

  # Optional: limit rows for testing
  max_rows: null

  # Categories to exclude from processing
  excluded_categories:
    - "Movies_and_TV"
    - "Software"
    - "Subscription_Boxes"
    - "Video_Games"
    - "Magazine_Subscriptions"
    - "Kindle_Store"
    - "Gift_Cards"
    - "Digital_Music"
    - "CDs_and_Vinyl"
    - "Books"

# ============================================================================
# Weekly Panel Builder
# ============================================================================
weekly_panel:
  # Quantile threshold for hot-seller label
  top_quantile: 0.95

  # Minimum rolling reviews to keep product
  min_reviews: 1

  # Rolling window for past reviews
  rolling_window: 4

  # Future window for growth calculation
  future_window: 12

# ============================================================================
# Transformer Model
# ============================================================================
transformer:
  # Sequence length (weeks)
  seq_len: 32

  # Model architecture
  d_model: 64
  nhead: 4
  num_layers: 3
  dim_feedforward: 128
  dropout: 0.1

  # Training
  batch_size: 256
  epochs: 10
  learning_rate: 0.001

  # Validation split (weeks)
  val_weeks: 26

# ============================================================================
# Forecast Pipeline
# ============================================================================
forecast:
  # Series and time columns
  series_col: "parent_asin"
  time_col: "week_start"
  target_col: "reviews"

  # Forecast horizon (weeks)
  horizon: 12

  # Validation size (weeks)
  val_size: 12

  # TFT Model
  tft:
    enabled: true
    max_encoder_len: 64
    hidden_size: 64
    attention_heads: 4
    dropout: 0.1
    learning_rate: 0.001
    max_epochs: 50
    batch_size: 128

  # AutoTS Model
  autots:
    enabled: true
    model_list: "fast"  # fast, superfast, or all
    ensemble: "simple"
    max_generations: 10
    num_validations: 2

  # Ensemble
  ensemble:
    method: "per_series"  # global or per_series
    metric: "smape"

# ============================================================================
# Slurm Configuration
# ============================================================================
slurm:
  # Job parameters
  partition: "main"
  nodes: 1
  ntasks: 1
  cpus_per_task: 32
  mem: "128G"
  time: "48:00:00"

  # Array job settings
  array_size: 23  # Number of categories

# ============================================================================
# Logging
# ============================================================================
logging:
  level: "INFO"  # DEBUG, INFO, WARNING, ERROR
  file: "pipeline.log"
  console: true

# ============================================================================
# Output & Artifacts
# ============================================================================
output:
  # Save predictions
  save_predictions: true

  # Save model checkpoints
  save_checkpoints: true

  # Generate plots
  generate_plots: true

  # Compression for parquet files
  compression: "zstd"